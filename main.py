import uvicorn
import google.generativeai as genai
import json
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List
from dotenv import load_dotenv
import asyncio
from concurrent.futures import ThreadPoolExecutor

# --- 1. CONFIGURA√á√ÉO ---
# Carrega vari√°veis de ambiente do arquivo .env
# Isso permite gerenciar chaves de API de forma segura sem hardcoding
load_dotenv()

# Obt√©m a chave da API do Google Gemini de vari√°vel de ambiente
# A chave √© necess√°ria para autenticar requisi√ß√µes √† API do Gemini
GOOGLE_AI_KEY = os.getenv("GOOGLE_AI_KEY")

if not GOOGLE_AI_KEY:
    raise ValueError(
        "GOOGLE_AI_KEY n√£o encontrada. "
        "Por favor, defina a vari√°vel de ambiente GOOGLE_AI_KEY ou crie um arquivo .env."
    )

try:
    # Configura a biblioteca do Google Generative AI com a chave fornecida
    # Esta configura√ß√£o √© global e aplica-se a todas as chamadas subsequentes
    genai.configure(api_key=GOOGLE_AI_KEY)
except Exception as e:
    raise RuntimeError(f"Erro ao configurar a API do Gemini: {e}")

# Thread pool para executar chamadas s√≠ncronas do Gemini de forma ass√≠ncrona
# Isso permite que o FastAPI mantenha sua natureza ass√≠ncrona enquanto
# executa opera√ß√µes bloqueantes do Gemini em threads separadas
# max_workers=5 permite at√© 5 requisi√ß√µes simult√¢neas ao Gemini
executor = ThreadPoolExecutor(max_workers=5)

# --- 2. DEFINI√á√ÉO DOS MODELOS DE DADOS (Contrato da API) ---
# O FastAPI usa Pydantic para validar automaticamente o JSON de entrada e sa√≠da.
# Isso garante type safety, valida√ß√£o de dados e documenta√ß√£o autom√°tica da API.

class Projeto(BaseModel):
    titulo: str
    descricao: str

class Perfil(BaseModel):
    id_perfil: int
    titulo_profissional: str
    resumo: str
    habilidades: List[str]

# Este √© o JSON que nossa API .NET vai enviar
class MatchRequest(BaseModel):
    projeto: Projeto
    perfis: List[Perfil]

# Este √© o formato de cada item que a IA vai devolver
class MatchResponseItem(BaseModel):
    id_perfil: int
    score_compatibilidade: int = Field(..., ge=0, le=100) # Garante que o score esteja entre 0-100
    justificativa: str

# Este √© o JSON final que nossa API de IA vai retornar para o .NET
class MatchResponse(BaseModel):
    matches: List[MatchResponseItem]

# --- 3. INICIALIZA√á√ÉO DA API ---
# FastAPI √© escolhido por sua performance, suporte nativo a async/await,
# documenta√ß√£o autom√°tica (Swagger/OpenAPI) e valida√ß√£o integrada com Pydantic
app = FastAPI(
    title="SkillSync AI Matchmaking API",
    description="Microservi√ßo de IA Generativa para fazer match entre projetos e freelancers usando Google Gemini.",
    version="1.0.0"
)

# --- 4. ENGENHARIA DE PROMPT (O "C√©rebro" da IA) ---
# Esta fun√ß√£o implementa t√©cnicas de Prompt Engineering para garantir
# respostas consistentes e estruturadas do modelo Gemini.
#
# Estrat√©gias utilizadas:
# 1. Contexto claro: Define o papel do assistente (especialista em RH)
# 2. Instru√ß√µes espec√≠ficas: Crit√©rios detalhados de an√°lise
# 3. Sistema de pontua√ß√£o: Escala objetiva de 0-100
# 4. Formato r√≠gido: Garante JSON v√°lido sem texto extra
def criar_prompt_matchmaking(request: MatchRequest) -> str:
    """
    Cria um prompt estruturado para o modelo Gemini realizar matchmaking.
    
    Args:
        request: Objeto MatchRequest contendo projeto e lista de perfis
        
    Returns:
        String com o prompt completo formatado para o Gemini
    """
    # Converte os dados de input em strings JSON formatadas
    # O indent=2 melhora a legibilidade do prompt para o modelo
    projeto_str = request.projeto.model_dump_json(indent=2)
    perfis_str = json.dumps([p.model_dump() for p in request.perfis], indent=2)

    # Prompt estruturado em 4 camadas (contexto, an√°lise, pontua√ß√£o, formato)
    # Este prompt foi engenheirado para maximizar consist√™ncia e qualidade
    return f"""Voc√™ √© um assistente de RH especialista em recrutamento de freelancers para a plataforma SkillSync.

Sua tarefa √© analisar um PROJETO e uma LISTA DE PERFIS de freelancers, e calcular a compatibilidade de cada perfil com o projeto.

PROJETO:
{projeto_str}

LISTA DE PERFIS:
{perfis_str}

INSTRU√á√ïES DE AN√ÅLISE:
1. Para CADA perfil na lista, analise:
   - Compatibilidade das habilidades listadas com os requisitos do projeto
   - Relev√¢ncia do t√≠tulo profissional
   - Adequa√ß√£o do resumo/experi√™ncia descrita
   - Alinhamento geral com o escopo do projeto

2. Atribua um score_compatibilidade de 0 a 100 para cada perfil:
   - 90-100: Perfil altamente compat√≠vel, atende todos os requisitos principais
   - 70-89: Perfil compat√≠vel, atende a maioria dos requisitos
   - 50-69: Perfil parcialmente compat√≠vel, pode atender com adapta√ß√µes
   - 30-49: Perfil pouco compat√≠vel, falta experi√™ncia/habilidades importantes
   - 0-29: Perfil incompat√≠vel, n√£o atende aos requisitos do projeto

3. Escreva uma justificativa curta (1-2 frases) explicando o score atribu√≠do.

FORMATO DE RESPOSTA OBRIGAT√ìRIO:
Voc√™ DEVE retornar APENAS um objeto JSON v√°lido, sem nenhum texto adicional, sem markdown, sem coment√°rios.
O JSON deve seguir EXATAMENTE este formato:

{{"matches": [{{"id_perfil": <n√∫mero>, "score_compatibilidade": <n√∫mero 0-100>, "justificativa": "<texto>"}}, ...]}}

IMPORTANTE: 
- Retorne APENAS o JSON, sem ```json ou qualquer outro texto
- Inclua TODOS os perfis da lista, mesmo que tenham score baixo
- Os campos devem ser exatamente: id_perfil, score_compatibilidade, justificativa
- score_compatibilidade deve ser um n√∫mero inteiro entre 0 e 100"""

# --- 5. O ENDPOINT DA API ---
# Este √© o √∫nico endpoint do nosso microservi√ßo
@app.post("/gerar-match", response_model=MatchResponse)
async def gerar_match(request: MatchRequest):
    """
    Recebe um Projeto e uma Lista de Perfis, e retorna os matches
    gerados pela IA Generativa (Gemini).
    """
    
    print("Recebida nova requisi√ß√£o de match...") # Log para o console

    # 1. Criar o Prompt
    prompt = criar_prompt_matchmaking(request)
    
    # 2. Configurar o modelo Gemini
    # Escolha do modelo: gemini-2.5-flash oferece bom balanceamento
    # entre velocidade, custo e qualidade para este caso de uso
    try:
        model = genai.GenerativeModel(
            model_name='gemini-2.5-flash',  # Modelo otimizado para velocidade e efici√™ncia
            generation_config={
                "response_mime_type": "application/json"  # For√ßa sa√≠da em JSON puro, sem markdown
            }
        )
    except Exception as e:
        print(f"Erro ao carregar o modelo Gemini: {e}")
        raise HTTPException(status_code=500, detail="Erro ao carregar o modelo de IA.")

    # 3. Chamar a IA Generativa (executando de forma ass√≠ncrona)
    raw_text = ""  # Inicializa para evitar erro em caso de exce√ß√£o
    try:
        print("Enviando prompt para o Gemini...")
        
        # Executa a chamada s√≠ncrona do Gemini em um thread pool
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(executor, model.generate_content, prompt)
        
        # 4. Processar a resposta da IA
        # Valida√ß√£o inicial: verifica se a resposta tem o formato esperado
        if not response or not hasattr(response, 'text'):
            raise ValueError("Resposta inv√°lida do modelo Gemini")
        raw_text = response.text.strip()
        
        # Limpeza de resposta: remove poss√≠veis markdown code blocks
        # Mesmo com response_mime_type, alguns modelos podem adicionar markdown
        # Esta etapa garante que temos apenas JSON puro para parsing
        if raw_text.startswith("```json"):
            raw_text = raw_text[7:]  # Remove ```json
        if raw_text.startswith("```"):
            raw_text = raw_text[3:]  # Remove ```
        if raw_text.endswith("```"):
            raw_text = raw_text[:-3]  # Remove ```
        raw_text = raw_text.strip()
        
        # 5. Converter o texto (string) em um objeto JSON (dicion√°rio Python)
        json_response = json.loads(raw_text)
        
        # 6. Validar que a resposta tem o formato esperado
        if "matches" not in json_response:
            raise ValueError("Resposta da IA n√£o cont√©m o campo 'matches'")
        
        # 7. Validar e normalizar matches
        matches = json_response["matches"]
        for match in matches:
            # Valida√ß√£o de estrutura: verifica campos obrigat√≥rios
            if "score_compatibilidade" not in match or "id_perfil" not in match or "justificativa" not in match:
                raise ValueError("Formato de match inv√°lido na resposta da IA")
            # Normaliza√ß√£o: garante que o score est√° sempre entre 0-100
            # Isso previne erros caso a IA retorne valores fora do range esperado
            match["score_compatibilidade"] = max(0, min(100, int(match["score_compatibilidade"])))
        
        # Ordena√ß√£o: ordena matches por score (maior primeiro)
        # Isso facilita a apresenta√ß√£o dos melhores matches primeiro
        matches.sort(key=lambda x: x["score_compatibilidade"], reverse=True)
        
        print(f"Match recebido do Gemini: {len(matches)} perfis analisados com sucesso.")
        return MatchResponse(matches=[MatchResponseItem(**match) for match in matches])

    except json.JSONDecodeError as e:
        error_msg = f"Erro: A IA n√£o retornou um JSON v√°lido."
        if raw_text:
            error_msg += f" Resposta recebida: {raw_text[:200]}..."  # Limita a 200 chars
        print(error_msg)
        raise HTTPException(
            status_code=500, 
            detail=f"A resposta da IA n√£o era um JSON v√°lido: {str(e)}"
        )
    except ValueError as e:
        print(f"Erro de valida√ß√£o: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro ao processar resposta da IA: {str(e)}")
    except Exception as e:
        print(f"Erro inesperado ao chamar a API do Gemini: {e}")
        raise HTTPException(status_code=500, detail=f"Erro interno do servidor de IA: {str(e)}")

# --- 6. ROTA DE HEALTH CHECK ---
# Boa pr√°tica para o .NET saber se a API de IA est√° "viva"
@app.get("/health")
def health_check():
    return {"status": "ok"}

# --- 7. RODAR LOCALMENTE ---
# Execute com: python main.py ou uvicorn main:app --reload
if __name__ == "__main__":
    print("üöÄ Iniciando servidor FastAPI local em http://127.0.0.1:8000")
    print("üìö Documenta√ß√£o interativa dispon√≠vel em http://127.0.0.1:8000/docs")
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)